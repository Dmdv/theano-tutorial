{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Intro (10 min)\n",
    "  * What is Theano and why should I care?\n",
    "  * Installation\n",
    "  * Configuration\n",
    "* Basics (30 min)\n",
    "  * Baby steps\n",
    "  * Theano variables and functions\n",
    "  * Shared variables and more on functions\n",
    "  * Debugging\n",
    "* Theano for Machine Learning (50 min)\n",
    "  * Logistic regression\n",
    "  * SVM\n",
    "  * Kernels\n",
    "  * Regularization\n",
    "* Lasagne (?? min)\n",
    "  * MLP\n",
    "  * Convolutional neural network\n",
    "  * Goodies (?)\n",
    "\n",
    "\n",
    "The code is here: https://github.com/dudevil/datafest-theano-tutorial/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/probably_theano.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project was started back in 2007 at the University of Montreal\n",
    "\n",
    "Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. \n",
    "\n",
    "But it's also much more than that:\n",
    "\n",
    "* Language for symbolic computation\n",
    "* Optimizing compiler\n",
    "* Python library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Dependencies:\n",
    "     \n",
    "   * System: **g++, BLAS**\n",
    "   * Python: **NumPy, SciPy** \n",
    "   * Goodies: **CUDA, CuDNN**\n",
    "\n",
    "\n",
    "Current release:\n",
    "```\n",
    "pip install Theano\n",
    "```\n",
    "\n",
    "Bleeding-edge:\n",
    "```\n",
    "pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    "```\n",
    "\n",
    "[Official instructions](http://deeplearning.net/software/theano/install.html#install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~/.theanorc\n",
    "\n",
    "```\n",
    "    [global]\n",
    "    device = gpu # cpu\n",
    "    floatX = float32\n",
    "    optimizer_including=cudnn \n",
    "    allow_gc = False # быстрее но использует больше памяти\n",
    "    #exception_verbosity=high\n",
    "    #optimizer = None  # полезно при отладке\n",
    "    #profile = True\n",
    "    #profile_memory = True\n",
    "\n",
    "    [nvcc]\n",
    "    fastmath = True\n",
    "```\n",
    "\n",
    "[More on configuration](http://deeplearning.net/software/theano/library/config.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano has great documentation:\n",
    "   * http://deeplearning.net/software/theano/tutorial/\n",
    "   * http://deeplearning.net/software/theano/index.html#documentation\n",
    "   \n",
    "Code samples:\n",
    "   * http://deeplearning.net/tutorial/\n",
    "   \n",
    "And user community:\n",
    "   * https://groups.google.com/forum/#!forum/theano-users\n",
    "\n",
    "Don't be afraid to peek into the code:\n",
    "   * https://github.com/Theano/Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baby steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano \n",
    "import theano.tensor as T\n",
    "\n",
    "%pylab inline\n",
    "figsize(8, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# doing stuff with theano\n",
    "\n",
    "# declare theano variable\n",
    "a = T.lscalar() \n",
    "\n",
    "# construct an expression\n",
    "expression = 1 + 2 * a + a ** 2 \n",
    "\n",
    "# compile a theano function\n",
    "f = theano.function(\n",
    "    inputs=[a],        # input\n",
    "    outputs=expression  # output\n",
    ")\n",
    "\n",
    "# evaluate the expression\n",
    "f(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theano variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only the declaration changes\n",
    "a = T.lvector()\n",
    "\n",
    "expression = 1 + 2 * a + a ** 2 \n",
    "\n",
    "f = theano.function(\n",
    "    inputs=[a],        # input\n",
    "    outputs=expression  # output\n",
    ")\n",
    "\n",
    "arg = arange(-10, 10)\n",
    "res = f(arg)\n",
    "\n",
    "plot(arg, res, c='m', linewidth=3.)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# can do the same with matrices\n",
    "x = T.dmatrix('x')\n",
    "y = T.dmatrix('y')\n",
    "\n",
    "z = x + 2 * y\n",
    "\n",
    "f = theano.function([x, y], z)\n",
    "f(ones((3, 3)), eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# broadcasting also works\n",
    "x = T.dmatrix('x')\n",
    "v = T.dvector('v')\n",
    "\n",
    "z = v + x\n",
    "\n",
    "f = theano.function([x, v], z)\n",
    "f(ones((3, 4)), ones((4,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# types should be handled with care\n",
    "x = T.fmatrix('x')\n",
    "v = T.fvector('v')\n",
    "\n",
    "z = v + x\n",
    "\n",
    "f = theano.function([x, v], z)\n",
    "print f(ones((3, 4), dtype=float32), np.ones((4,), dtype=float32))\n",
    "print f(ones((3, 4)), ones((4,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or you can supress the exception\n",
    "x = T.fmatrix('x')\n",
    "v = T.fvector('v')\n",
    "\n",
    "z = v + x\n",
    "\n",
    "f = theano.function(\n",
    "    inputs=[x, v],\n",
    "    outputs=z,\n",
    "    allow_input_downcast=True\n",
    ")\n",
    "print f(ones((3, 4), dtype=float64), ones((4,), dtype=float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared variables and more on functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shared variables represent internal state\n",
    "state = theano.shared(0)\n",
    "\n",
    "i = T.iscalar('i')\n",
    "inc = theano.function([i],\n",
    "                      state,\n",
    "                      # updates the shared variable value\n",
    "                      updates=[(state, state+i)]) \n",
    "dec = theano.function([i],\n",
    "                      state,\n",
    "                      updates=[(state, state-i)])\n",
    "\n",
    "# more than one function can update the shared variable\n",
    "print state.get_value()\n",
    "inc(1)\n",
    "inc(1)\n",
    "inc(1)\n",
    "print state.get_value()\n",
    "dec(2)\n",
    "print state.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can also set the shared variable outside of the function\n",
    "state.set_value(-15)\n",
    "print state.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shared variables can in be inserted into the compurational graph\n",
    "x = T.lscalar('x')\n",
    "y = T.lscalar('y')\n",
    "i = T.lscalar('i')\n",
    "\n",
    "expression = (x - y) ** 2\n",
    "\n",
    "state = theano.shared(0)\n",
    "\n",
    "f = theano.function(\n",
    "    inputs=[x, i],\n",
    "    outputs=expression,\n",
    "    updates=[(state, state+i)],\n",
    "    # use the value at state as y\n",
    "    givens={\n",
    "        y : state\n",
    "    }\n",
    ")\n",
    "print f(5, 1) # (5 - 0) ^ 2 = 25\n",
    "print f(2, 1) # (2 - 1) ^ 2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can compute different outputs simultaneously\n",
    "x = T.lscalar('x')\n",
    "y = T.lscalar('y')\n",
    "\n",
    "square = T.square(x + y)\n",
    "sqrt = T.sqrt(x + y)\n",
    "\n",
    "f = theano.function(\n",
    "    inputs=[x, y],\n",
    "    outputs=[square, sqrt]\n",
    ")\n",
    "print f(5, 4)\n",
    "print f(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can compute different outputs simultaneously\n",
    "x = T.lscalar('x')\n",
    "y = T.lscalar('y')\n",
    "\n",
    "square = T.square(x + y)\n",
    "sqrt = T.sqrt(x + y)\n",
    "\n",
    "f = theano.function(\n",
    "    inputs=[x, y],\n",
    "    outputs=[square, sqrt]\n",
    ")\n",
    "# the sum get's computed only once\n",
    "theano.printing.debugprint(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a theano expression\n",
    "W = T.fmatrix('W')\n",
    "b = T.fvector('b')\n",
    "X = T.fmatrix('X')\n",
    "\n",
    "expr = T.dot(X, W) + b\n",
    "prob = 1 / (1 + T.exp(-expr))\n",
    "pred = prob > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and print it\n",
    "theano.pprint(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.printing.debugprint(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.printing.pydotprint(pred, outfile='pics/pred_graph.png', var_with_name_simple=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/pred_graph.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a theano expression\n",
    "W_1 = T.fmatrix('W_1')\n",
    "W_2 = T.fmatrix('W_2')\n",
    "b_1 = T.fvector('b_1')\n",
    "b_2 = T.fvector('b_2')\n",
    "activation = lambda expr: 1 / (1 + T.exp(-expr))\n",
    "X = T.fmatrix('X')\n",
    "y = T.ivector('y')\n",
    "\n",
    "d1 = T.dot(X, W_1) + b_1\n",
    "a1 = activation(d1)\n",
    "d2 = T.dot(a1, W_2) + b_2\n",
    "prob = activation(d2)\n",
    "loss = T.nnet.categorical_crossentropy(prob, y)\n",
    "\n",
    "theano.printing.pydotprint(loss, outfile='pics/pred_biggraph.png', var_with_name_simple=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/pred_biggraph.png\">\n",
    "<img src=\"pics/escalated_quickly.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on graph visualizations: http://deeplearning.net/software/theano/tutorial/printing_drawing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonitorMode for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inspect_inputs(i, node, fn):\n",
    "    print i, node, \"input(s) value(s):\", [input[0] for input in fn.inputs],\n",
    "\n",
    "def inspect_outputs(i, node, fn):\n",
    "    print \"output(s) value(s):\", [output[0] for output in fn.outputs]\n",
    "\n",
    "x = theano.tensor.dscalar('x')\n",
    "f = theano.function(inputs=[x], \n",
    "                    outputs=(5 * x),\n",
    "                    mode=theano.compile.MonitorMode(\n",
    "                        pre_func=inspect_inputs,\n",
    "                        post_func=inspect_outputs))\n",
    "f(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More debugging technics are available here: http://deeplearning.net/software/theano/tutorial/debug_faq.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano for Machine learining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = linspace(-1, 1, 100)\n",
    "x2 = 1.5 - x1 ** 2 + random.normal(scale=0.2, size=100)\n",
    "x3 = random.normal(scale=0.4, size=100)\n",
    "x4 = random.normal(scale=0.4, size=100)\n",
    "\n",
    "permutation = random.permutation(np.arange(200))\n",
    "x = hstack((\n",
    "    vstack((x1, x2)),\n",
    "    vstack((x3, x4)))).T[permutation]\n",
    "y = concatenate((\n",
    "    zeros_like(x1),\n",
    "    ones_like(x3)))[permutation]\n",
    "\n",
    "# needed for pictures later\n",
    "xx, yy = mgrid[-2:2:.01, -2:2:.01]\n",
    "grid_arr = c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "def plot_decision(predicts):\n",
    "    probas = predicts.reshape(xx.shape)\n",
    "\n",
    "    contour = contourf(xx, yy, probas, 25, cmap=\"RdBu\", vmin=0, vmax=1)\n",
    "    colorbar(contour)\n",
    "\n",
    "    scatter(x[:,0], x[:, 1], c=y, s=50,\n",
    "                cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "                edgecolor=\"white\", linewidth=1)\n",
    "    title(\"Some cool decision boundary\")\n",
    "    grid()\n",
    "    \n",
    "scatter(x[:,0], x[:, 1], c=y, s=75,\n",
    "            cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "            edgecolor=\"white\", linewidth=1)\n",
    "title(\"Toy data\")\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# allocate parameters\n",
    "W = theano.shared(\n",
    "    value=numpy.zeros((2, 1),dtype=theano.config.floatX), \n",
    "    name='W',  \n",
    "    borrow=True)\n",
    "\n",
    "b = theano.shared(\n",
    "    value=numpy.zeros((1,), dtype=theano.config.floatX),\n",
    "    name='b',\n",
    "    borrow=True)\n",
    "\n",
    "# and define symbolic variables\n",
    "X = T.matrix('X')\n",
    "Y = T.imatrix('Y')\n",
    "\n",
    "# define model\n",
    "linear = T.dot(X, W) + b\n",
    "p_y_given_x = T.nnet.sigmoid(linear)\n",
    "y_pred = p_y_given_x > 0.5\n",
    "\n",
    "# define loss-function\n",
    "loss = T.nnet.binary_crossentropy(p_y_given_x, Y).mean()\n",
    "\n",
    "# compute the gradients\n",
    "g_W = T.grad(loss, W)\n",
    "g_b = T.grad(loss, b)\n",
    "\n",
    "# define parametes updates\n",
    "updates = [(W, W - 0.04 * g_W),\n",
    "           (b, b - 0.08 * g_b)]\n",
    "\n",
    "# compile functions\n",
    "train = theano.function(\n",
    "    inputs=[X, Y],\n",
    "    outputs=loss,\n",
    "    updates=updates,\n",
    "    allow_input_downcast=True\n",
    ")\n",
    "\n",
    "predict_proba = theano.function(\n",
    "    [X],\n",
    "    p_y_given_x,\n",
    "    allow_input_downcast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## SGD is love SGD is life\n",
    "batch_size = 4\n",
    "\n",
    "for epoch_ in xrange(301):\n",
    "    loss = []\n",
    "    # iterate over training samples in minibatches\n",
    "    for iter_ in xrange(x.shape[0] // batch_size):\n",
    "        minibatch = slice(iter_ * batch_size, (iter_ + 1) * batch_size)\n",
    "        loss.append(train(x[minibatch], y[minibatch, np.newaxis]))\n",
    "    \n",
    "    e_loss = mean(loss)\n",
    "    if not epoch_ % 10:\n",
    "        print(\"[Epoch %03d] Train loss: %f\" % (epoch_, e_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas = predict_proba(grid_arr)\n",
    "plot_decision(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "W = theano.shared(\n",
    "    value=numpy.zeros((2, 1),dtype=theano.config.floatX), \n",
    "    name='W',  \n",
    "    borrow=True)\n",
    "\n",
    "b = theano.shared(\n",
    "    value=numpy.zeros((1,), dtype=theano.config.floatX),\n",
    "    name='b',\n",
    "    borrow=True)\n",
    "\n",
    "# and define symbolic variables\n",
    "X = T.matrix('X')\n",
    "Y = T.imatrix('Y')\n",
    "\n",
    "# define model\n",
    "linear = T.dot(X, W) + b\n",
    "\n",
    "# We only need to change the loss function\n",
    "loss = T.maximum(0, 1 - linear * (Y * 2 - 1)).mean()\n",
    "\n",
    "# compute the gradients\n",
    "g_W = T.grad(loss, W)\n",
    "g_b = T.grad(loss, b)\n",
    "\n",
    "# define parametes updates\n",
    "updates = [(W, W - 0.04 * g_W),\n",
    "           (b, b - 0.08 * g_b)]\n",
    "\n",
    "# compile functions\n",
    "train = theano.function(\n",
    "    inputs=[X, Y],\n",
    "    outputs=[loss],\n",
    "    updates=updates,\n",
    "    allow_input_downcast=True\n",
    ")\n",
    "               \n",
    "predict = theano.function(\n",
    "    [X],\n",
    "    linear > 0,\n",
    "    allow_input_downcast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch_ in xrange(301):\n",
    "    loss = []\n",
    "    # iterate over training samples in minibatches\n",
    "    for iter_ in xrange(x.shape[0] // batch_size):\n",
    "        minibatch = slice(iter_ * batch_size, (iter_ + 1) * batch_size)\n",
    "        loss.append(train(x[minibatch], y[minibatch, np.newaxis]))\n",
    "    \n",
    "    e_loss = mean(loss)\n",
    "    if not epoch_ % 10:\n",
    "        print(\"[Epoch %03d] Train loss: %f\" % (epoch_, e_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = predict(grid_arr)\n",
    "plot_decision(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Kernel\" trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# meet theano's scan:\n",
    "# allocate variable\n",
    "i = T.lscalar(\"i\")\n",
    "\n",
    "# fn parameters: sequences (if any), prior result(s) (if needed), non-sequences (if any)\n",
    "# the sequences argument is ommited in this example\n",
    "result, updates = theano.scan(fn=lambda prior_result, i: prior_result * i,\n",
    "                              # initialize the output\n",
    "                              outputs_info=T.ones_like(i),\n",
    "                              # pass input as non sequence\n",
    "                              non_sequences=i,\n",
    "                              # this many iterations\n",
    "                              n_steps=3)\n",
    "\n",
    "# compile the function\n",
    "poly = theano.function(inputs=[i], \n",
    "                        outputs=result,\n",
    "                        updates=updates, # actually safe to omit in this case\n",
    "                        allow_input_downcast=True)\n",
    "poly(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# meet theano's scan 2:\n",
    "# allocate variable\n",
    "I = T.lmatrix(\"I\")\n",
    "\n",
    "# fn parameters: sequences (if any), prior result(s) (if needed), non-sequences (if any)\n",
    "# the sequences argument is ommited in this example\n",
    "result, updates = theano.scan(fn=lambda prior_result, I: prior_result * I,\n",
    "                              # initialize the output\n",
    "                              outputs_info=T.ones_like(I),\n",
    "                              # pass input as non sequence\n",
    "                              non_sequences=I,\n",
    "                              # this many iterations\n",
    "                              n_steps=2)\n",
    "\n",
    "# x  y  -> x  y  x^2  y^2\n",
    "# x' y' -> x' y' x'^2 y'^2\n",
    "\n",
    "output = result.dimshuffle(1, 0, 2).reshape((result.shape[1], \n",
    "                                             result.shape[0] * result.shape[2]))\n",
    "# compile the function\n",
    "poly = theano.function(inputs=[I], \n",
    "                        outputs=output,\n",
    "                        updates=updates, # actually safe to omit in this case\n",
    "                        allow_input_downcast=True)\n",
    "output = poly((arange(6) + 1).reshape(3, 2))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More documentation on looping in Theano: http://deeplearning.net/software/theano/library/scan.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "def poly(inp, degree=2):\n",
    "  \n",
    "    result, updates = theano.scan(fn=lambda prior_result, inp: prior_result * inp,\n",
    "                                  # initialize the output\n",
    "                                  outputs_info=T.ones_like(inp),\n",
    "                                  # pass input as non sequence\n",
    "                                  non_sequences=inp,\n",
    "                                  # this many iterations\n",
    "                                  n_steps=degree)\n",
    "    return result.dimshuffle(1, 0, 2).reshape((result.shape[1], \n",
    "                                               result.shape[0] * result.shape[2]))\n",
    "\n",
    "# reset parameters\n",
    "W = theano.shared(\n",
    "    value=numpy.zeros((8, 1),dtype=theano.config.floatX), \n",
    "    name='W',  \n",
    "    borrow=True)\n",
    "\n",
    "b = theano.shared(\n",
    "    value=numpy.zeros((1,), dtype=theano.config.floatX),\n",
    "    name='b',\n",
    "    borrow=True)\n",
    "# and define symbolic variables\n",
    "X = T.matrix('X')\n",
    "Y = T.imatrix('Y')\n",
    "\n",
    "# define model\n",
    "linear = T.dot(poly(X, degree=2), W) + b\n",
    "\n",
    "# We only need to change the loss function\n",
    "loss = T.maximum(0, 1 - linear * (Y * 2 - 1)).mean()\n",
    "\n",
    "# compute the gradients\n",
    "g_W = T.grad(loss, W)\n",
    "g_b = T.grad(loss, b)\n",
    "\n",
    "# define parametes updates\n",
    "updates = [(W, W - 0.04 * g_W),\n",
    "           (b, b - 0.08 * g_b)]\n",
    "\n",
    "# compile functions\n",
    "train = theano.function(\n",
    "    inputs=[X, Y],\n",
    "    outputs=[loss],\n",
    "    updates=updates,\n",
    "    allow_input_downcast=True\n",
    ")\n",
    "               \n",
    "predict = theano.function(\n",
    "    [X],\n",
    "    linear > 0,\n",
    "    allow_input_downcast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "for epoch_ in xrange(301):\n",
    "    loss = []\n",
    "    # iterate over training samples in minibatches\n",
    "    for iter_ in xrange(x.shape[0] // batch_size):\n",
    "        minibatch = slice(iter_ * batch_size, (iter_ + 1) * batch_size)\n",
    "        loss.append(train(x[minibatch], y[minibatch, np.newaxis]))\n",
    "    \n",
    "    e_loss = mean(loss)\n",
    "    if not epoch_ % 10:\n",
    "        print(\"[Epoch %03d] Train loss: %f\" % (epoch_, e_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = predict(grid_arr)\n",
    "plot_decision(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "W = theano.shared(\n",
    "    value=zeros((8, 1),dtype=theano.config.floatX), \n",
    "    name='W',  \n",
    "    borrow=True)\n",
    "\n",
    "b = theano.shared(\n",
    "    value=zeros((1,), dtype=theano.config.floatX),\n",
    "    name='b',\n",
    "    borrow=True)\n",
    "# and define symbolic variables\n",
    "X = T.matrix('X')\n",
    "Y = T.imatrix('Y')\n",
    "\n",
    "# define model\n",
    "linear = T.dot(poly(X, degree=4), W) + b\n",
    "\n",
    "# We only need to change the loss function\n",
    "loss = T.maximum(0, 1 - linear * (Y * 2 - 1)).mean() + 1e-3 * T.sum(W ** 2)\n",
    "\n",
    "# compute the gradients\n",
    "g_W = T.grad(loss, W)\n",
    "g_b = T.grad(loss, b)\n",
    "\n",
    "# define parametes updates\n",
    "updates = [(W, W - 0.04 * g_W),\n",
    "           (b, b - 0.08 * g_b)]\n",
    "\n",
    "# compile functions\n",
    "train = theano.function(\n",
    "    inputs=[X, Y],\n",
    "    outputs=loss,\n",
    "    updates=updates,\n",
    "    allow_input_downcast=True\n",
    ")\n",
    "               \n",
    "predict = theano.function(\n",
    "    inputs=[X],\n",
    "    linear > 0,\n",
    "    allow_input_downcast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "for epoch_ in xrange(301):\n",
    "    loss = []\n",
    "    # iterate over training samples in minibatches\n",
    "    for iter_ in xrange(x.shape[0] // batch_size):\n",
    "        minibatch = slice(iter_ * batch_size, (iter_ + 1) * batch_size)\n",
    "        loss.append(train(x[minibatch], y[minibatch, np.newaxis]))\n",
    "    \n",
    "    e_loss = mean(loss)\n",
    "    if not epoch_ % 10:\n",
    "        print(\"[Epoch %03d] Train loss: %f\" % (epoch_, e_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = predict(grid_arr)\n",
    "plot_decision(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasagne is a lightweight library to build and train neural networks in Theano:\n",
    "\n",
    "https://github.com/Lasagne/Lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.utils import floatX\n",
    "\n",
    "import gzip\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget -P data http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "!wget -P data  http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the following code has been mostly adopted from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\n",
    "# you are encouraged to go through the example there or have a look at a more in-depth tutorial: \n",
    "# http://lasagne.readthedocs.org/en/latest/user/tutorial.html\n",
    "\n",
    "def plot_mnist_sample(sample):\n",
    "    imshow(sample[0], cmap=cm.Greys_r)\n",
    "    xticks([])\n",
    "    yticks([])\n",
    "\n",
    "with gzip.open(\"data/train-images-idx3-ubyte.gz\", 'rb') as f:\n",
    "    X = frombuffer(f.read(), uint8, offset=16).reshape(-1, 1, 28, 28)\n",
    "    X = X / floatX(256)\n",
    "    \n",
    "with gzip.open(\"data/train-labels-idx1-ubyte.gz\", 'rb') as f:\n",
    "    y = frombuffer(f.read(), uint8, offset=8)\n",
    "    \n",
    "X_train, X_val = X[:-10000], X[-10000:]\n",
    "y_train, y_val = y[:-10000], y[-10000:]\n",
    "\n",
    "plot_mnist_sample(X_train[randint(0, 10000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_var=None):\n",
    "    # This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "    # a softmax output layer of 10 units. It applies 20% dropout to the input\n",
    "    # data and 50% dropout to the hidden layers.\n",
    "\n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    # (unspecified batchsize, 1 channel, 28 rows and 28 columns) and\n",
    "    # linking it to the given Theano variable `input_var`, if any:\n",
    "    network = lasagne.layers.InputLayer(\n",
    "        shape=(None, 1, 28, 28),\n",
    "        input_var=input_var)\n",
    "    \n",
    "    # Apply 20% dropout to the input data:\n",
    "    network = lasagne.layers.DropoutLayer(network, p=0.2)\n",
    "\n",
    "    # Add a fully-connected layer of 800 units, using the linear rectifier, and\n",
    "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "        network, \n",
    "        num_units=800,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    # We'll now add dropout of 50%:\n",
    "    network = lasagne.layers.DropoutLayer(network, p=0.5)\n",
    "\n",
    "    # Another 800-unit layer:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "        network,\n",
    "        num_units=800,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # 50% dropout again:\n",
    "    network = lasagne.layers.DropoutLayer(network, p=0.5)\n",
    "\n",
    "    # Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "        network,\n",
    "        num_units=10,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "    \n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "        network, \n",
    "        num_filters=32,\n",
    "        filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "        network,\n",
    "        num_filters=32,\n",
    "        filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = arange(len(inputs))\n",
    "        random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# choose MLP or CNN\n",
    "network = build_mlp(input_var)\n",
    "#network = build_cnn(input_var)\n",
    "\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "# We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "    loss, \n",
    "    params, \n",
    "    learning_rate=0.01, \n",
    "    momentum=0.9)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = T.nnet.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(\n",
    "    T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "    dtype=theano.config.floatX)\n",
    "\n",
    "train = theano.function(\n",
    "    inputs=[input_var, target_var],\n",
    "    outputs=loss,\n",
    "    updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "validate = theano.function(\n",
    "    inputs=[input_var, target_var],\n",
    "    outputs=[test_loss, test_acc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"| Epoch | Train err | Validation err | Accuracy |  Time  |\")\n",
    "print(\"|--------------------------------------------------------|\")\n",
    "\n",
    "try:\n",
    "    for epoch in xrange(500):\n",
    "            # In each epoch, we do a full pass over the training data:\n",
    "            train_err = 0\n",
    "            train_batches = 0\n",
    "            start_time = time.time()\n",
    "            for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "                inputs, targets = batch\n",
    "                train_err += train(inputs, targets)\n",
    "                train_batches += 1\n",
    "\n",
    "            # And a full pass over the validation data:\n",
    "            val_err = 0\n",
    "            val_acc = 0\n",
    "            val_batches = 0\n",
    "            for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "                inputs, targets = batch\n",
    "                err, acc = validate(inputs, targets)\n",
    "                val_err += err\n",
    "                val_acc += acc\n",
    "                val_batches += 1\n",
    "\n",
    "            print(\"|%6d | %9.6f | %14.6f | %8.2f | %6d |\" %\n",
    "                            (epoch,\n",
    "                             train_err / train_batches,\n",
    "                             val_err / val_batches,\n",
    "                             val_acc / val_batches * 100,\n",
    "                             time.time() - start_time))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"The training was interrupted on epoch: %d.\" % epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the weights\n",
    "savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "\n",
    "network = build_mlp()\n",
    "# And load them again later on like this:\n",
    "with np.load('model.npz') as f:\n",
    "     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting predictions out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the expression\n",
    "predictions = T.argmax(test_prediction, axis=1)\n",
    "\n",
    "# define the prediction function\n",
    "predict = theano.function(\n",
    "    inputs=[input_var],\n",
    "    outputs=predictions)\n",
    "\n",
    "# get just one batch for simplicity\n",
    "inputs, targets = iterate_minibatches(X_val, y_val, 500, shuffle=False).next()\n",
    "preds = predict(inputs)\n",
    "bad_samples = where(preds != targets)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = random.choice(bad_samples)\n",
    "plot_mnist_sample(X_val[error])\n",
    "print(\"Predicted: %d True: %d\" % (preds[error], targets[error]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of interesting stuff here: https://github.com/Lasagne/Recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for your attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
